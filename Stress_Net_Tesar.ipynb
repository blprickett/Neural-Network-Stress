{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Stress-Net-Tesar.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zZr8ReFcNBEi"
      },
      "source": [
        "##Hyperparameters for the model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CTlg8GYqplLc",
        "outputId": "656a463f-6101-4d33-c94f-9e8fe319c656"
      },
      "source": [
        "epoch_num = 500 #Number of epochs\n",
        "learning_rate = 0.0005\n",
        "hidden_feat_num = 20\n",
        "batch_size = 1\n",
        "lang_list = range(40,60) #eventually make it to 124...\n",
        "token_per_type = 1\n",
        "\n",
        "#Dictionary that maps from feature values in the SR to segments:\n",
        "feats2symbol_sr = {\n",
        "                      \"1,1,1\":\"1\",\n",
        "                      \"1,1,-1\":\"2\",\n",
        "                      \"1,-1,-1\":\"0\",\n",
        "                      \"-1,-1,-1\":\"E\" \n",
        "                  }\n",
        "\n",
        "#Mount the user's google drive:\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4kHGQQcmM9nZ"
      },
      "source": [
        "##Install and import the necessary packages:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yJFXBk8wVRwQ",
        "outputId": "f4abded3-377e-42a6-fe5a-e8eb9093e92d"
      },
      "source": [
        "#CD into our google drive so we can use the custom Seq2Seq package\n",
        "%cd gdrive/My Drive\n",
        "import Seq2Seq\n",
        "\n",
        "#Import everything else:\n",
        "import numpy as np\n",
        "from re import sub\n",
        "from tensorflow.keras import backend\n",
        "from random import choice, shuffle\n",
        "from itertools import product\n",
        "from matplotlib.pyplot import plot\n",
        "from IPython.display import clear_output\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RiyA_Je6NIdU"
      },
      "source": [
        "##Functions for processing the training data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gWYjgS_MqcFX"
      },
      "source": [
        "def get_strings(input_fn, copies=1):\n",
        "  input_file = open(\"/content/gdrive/My Drive/NN_Stress/Input_Files/\"+input_fn)\n",
        "  input_file.readline()\n",
        "  UR_strings = []\n",
        "  SR_strings = []\n",
        "  syll_lengths = []\n",
        "  for line in input_file.readlines():\n",
        "    columns = line.split(\",\")\n",
        "    if len(columns) == 1:\n",
        "      raw_ur = line.rstrip() \n",
        "    if len(columns) == 3:\n",
        "      raw_sr = columns[1]\n",
        "      raw_p = columns[2]\n",
        "      ur, sr, p = sub(\"\\\"|\\[|\\]\", \"\", raw_ur), sub(\"\\[|\\]\", \"\", raw_sr), raw_p.rstrip()\n",
        "      if p == \"1\":\n",
        "        syll_lengths.append(len(ur.split(\" \")))\n",
        "        UR_strings.append(ur)\n",
        "        new_sr = []\n",
        "        for syll in sr.split(\" \"):\n",
        "          new_syll = {\"L1\":\"1\", \"H1\":\"1\", \"L2\":\"2\", \"H2\":\"2\", \"L\":\"0\", \"H\":\"0\"}[syll]\n",
        "          new_sr.append(new_syll)\n",
        "        SR_strings.append(\" \".join(new_sr))\n",
        "\n",
        "  if copies > 1:\n",
        "    lexical_labels = [c for c in range(1,1+(copies*len(UR_strings)))]\n",
        "    shuffle(lexical_labels)\n",
        "    token_UR_strings = []\n",
        "    token_SR_strings = []\n",
        "    token_syll_lengths = []\n",
        "    for i, ur in enumerate(UR_strings):\n",
        "      sr = SR_strings[i]\n",
        "      for copy in range(copies):\n",
        "        this_label = str(lexical_labels.pop())\n",
        "        token_UR_strings.append(ur+\"_\"+this_label)\n",
        "        token_SR_strings.append(sr)\n",
        "        token_syll_lengths.append(syll_lengths[i])\n",
        "    return token_UR_strings, token_SR_strings, token_syll_lengths\n",
        "\n",
        "  return UR_strings, SR_strings, syll_lengths\n",
        "\n",
        "def get_arrays(UR_strings, SR_strings, syll_lengths):\n",
        "  symbol2feats_ur = {\n",
        "                  #Syll\t#Heavy\n",
        "              \"L\":[\t1.,\t\t-1.],\n",
        "              \"H\":[\t1.,\t\t1.],\n",
        "              \"E\":[\t-1.,\t\t-1.]\n",
        "            }\n",
        "  symbol2feats_sr = {}\n",
        "  for feat_bundle in feats2symbol_sr.keys():\n",
        "    these_feats = [float(f) for f in feat_bundle.split(\",\")]\n",
        "    symbol2feats_sr[feats2symbol_sr[feat_bundle]] = these_feats\n",
        "\n",
        "  if \"_\" in \"\".join(UR_strings):\n",
        "    #If we're using lexical features:\n",
        "    real_strings = []\n",
        "    raw_lex_nums = []\n",
        "    max_lexLen = -1\n",
        "    ur_pieces = ur.split(\"_\")\n",
        "    if len(ur_pieces) > 1:\n",
        "      real_string, lex_num = ur_pieces\n",
        "      bin_lex_num = str(bin(int(lex_num)).replace(\"0b\", \"\"))\n",
        "      raw_lex_nums.append(bin_lex_num)\n",
        "      if len(bin_lex_num) > max_lexLen:\n",
        "        max_lexLen = len(bin_lex_num)\n",
        "    else:\n",
        "      real_string = ur\n",
        "    real_strings.append(real_string)\n",
        "\n",
        "    lex_nums = []\n",
        "    for ln in raw_lex_nums:\n",
        "      zeros = \"0\" * (max_lexLen - len(ln))\n",
        "      lex_nums.append([float(digit) for digit in zeros+ln])\n",
        "\n",
        "    max_len = max(syll_lengths)\n",
        "    X_list = []\n",
        "    Y_list = []\n",
        "    for word_index, syll_length in enumerate(syll_lengths):\n",
        "      padding = \" \".join([\"E\"]*(max_len-syll_length))\n",
        "      this_length = len(real_strings[word_index].split(\" \"))\n",
        "      this_ur = real_strings[word_index]+\" \"+padding\n",
        "      this_sr = SR_strings[word_index]+\" \"+padding\n",
        "      if len(lex_nums) > 0:\n",
        "        X_list.append([symbol2feats_ur[seg]+lex_nums[word_index] for seg in this_ur.split(\" \") if seg != \"\"])\n",
        "      else:\n",
        "        X_list.append([symbol2feats_ur[seg] for seg in this_ur.split(\" \") if seg != \"\"])\n",
        "      Y_list.append([symbol2feats_sr[seg] for seg in this_sr.split(\" \") if seg != \"\"])\n",
        "  else:\n",
        "    max_len = max(syll_lengths)\n",
        "    X_list = []\n",
        "    Y_list = []\n",
        "    for word_index, syll_length in enumerate(syll_lengths):\n",
        "      padding = \" \".join([\"E\"]*(max_len-syll_length))\n",
        "      this_length = len(UR_strings[word_index].split(\" \"))\n",
        "      this_ur = UR_strings[word_index]+\" \"+padding\n",
        "      this_sr = SR_strings[word_index]+\" \"+padding\n",
        "      X_list.append([symbol2feats_ur[seg] for seg in this_ur.split(\" \") if seg != \"\"])\n",
        "      Y_list.append([symbol2feats_sr[seg] for seg in this_sr.split(\" \") if seg != \"\"])\n",
        "\n",
        "  X = np.array(X_list)\n",
        "  Y = np.array(Y_list)\n",
        "\n",
        "  return X, Y\n",
        "\n",
        "\n",
        "def get_test_arrays (UR_strings, syll_lengths):\n",
        "  symbol2feats_ur = {\n",
        "                #Syll\t#Heavy\n",
        "            \"L\":[\t1.,\t\t-1.],\n",
        "            \"H\":[\t1.,\t\t1.],\n",
        "            \"E\":[\t-1.,\t\t-1.]\n",
        "          }\n",
        "  real_strings = []\n",
        "  raw_lex_nums = []\n",
        "  max_lexLen = -1\n",
        "  for ur in UR_strings:\n",
        "    ur_pieces = ur.split(\"_\")\n",
        "    if len(ur_pieces) > 1:\n",
        "      real_string, lex_num = ur_pieces\n",
        "      bin_lex_num = str(bin(int(lex_num)).replace(\"0b\", \"\"))\n",
        "      raw_lex_nums.append(bin_lex_num)\n",
        "      if len(bin_lex_num) > max_lexLen:\n",
        "        max_lexLen = len(bin_lex_num)\n",
        "    else:\n",
        "      real_string = ur\n",
        "    real_strings.append(real_string)\n",
        "\n",
        "  lex_nums = []\n",
        "  for ln in raw_lex_nums:\n",
        "    zeros = \"0\" * (max_lexLen - len(ln))\n",
        "    lex_nums.append([0.0 for digit in zeros+ln]) #All zeroes for lexical features, b/c test data are nonce words\n",
        "\n",
        "  max_len = max(syll_lengths)\n",
        "  X_list = []\n",
        "  for word_index, syll_length in enumerate(syll_lengths):\n",
        "    padding = \" \".join([\"E\"]*(max_len-syll_length))\n",
        "    this_length = len(real_strings[word_index].split(\" \"))\n",
        "    this_ur = real_strings[word_index]+\" \"+padding\n",
        "    if len(lex_nums) > 0:\n",
        "      X_list.append([symbol2feats_ur[seg]+lex_nums[word_index] for seg in this_ur.split(\" \") if seg != \"\"])\n",
        "    else:\n",
        "      X_list.append([symbol2feats_ur[seg] for seg in this_ur.split(\" \") if seg != \"\"])\n",
        "\n",
        "  return np.array(X_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gcw6CW8oNPPv"
      },
      "source": [
        "##Run the simulations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Xez-20vqckP"
      },
      "source": [
        "learning_curves = []\n",
        "lang2results = {l:{} for l in lang_list}\n",
        "lang2testResults = {l:{} for l in lang_list}\n",
        "lang2acc = {l:0 for l in lang_list}\n",
        "lang2SRs = {}\n",
        "for l in lang_list:\n",
        "  print (\"Language\", l)\n",
        "\n",
        "  #Load the training data:\n",
        "  file_name = \"ts\"+str(l)+\".csv\"\n",
        "  URs, SRs, Ls = get_strings(file_name, token_per_type)\n",
        "\n",
        "  lang2SRs[l] = SRs\n",
        "\n",
        "  X, Y = get_arrays(URs, SRs, Ls)\n",
        "  test_X = get_test_arrays(URs, Ls)\n",
        "  lang2results[l] = {ur:\"\" for ur in URs}\n",
        "  lang2testResults[l] = {sub(\"_.+\", \"\", ur):\"\" for ur in URs}\n",
        "\n",
        "  #Build the model:\n",
        "  model = Seq2Seq.seq2seq(\n",
        "                              input_dim=X.shape[2],\n",
        "                              hidden_dim=hidden_feat_num,\n",
        "                              output_length=Y.shape[1],\n",
        "                              output_dim=Y.shape[2],\n",
        "                              batch_size=batch_size,\n",
        "                              learn_rate=learning_rate\n",
        "                            )\n",
        "\t\n",
        "  #Train the model:\n",
        "  hist = model.train(\n",
        "                        X, Y,\n",
        "                        epoch_num=epoch_num,\n",
        "                        print_every=10\n",
        "                     )\n",
        "  \n",
        "  #Save the useful info and delete everything else:\n",
        "  #Training data performance:\n",
        "  learning_curves.append(hist[\"Loss\"])\n",
        "  Y_hat = model.predict(X)\n",
        "  accs_by_word = []\n",
        "  for i, word in enumerate(Y_hat):\n",
        "    word_list = []\n",
        "    for seg in word:\n",
        "      feat_strings = []\n",
        "      for feat in seg:\n",
        "        if feat < 0.0:\n",
        "          feat_strings.append(\"-1\")\n",
        "        elif feat > 0.0:\n",
        "          feat_strings.append(\"1\")\n",
        "        else:\n",
        "          raise Exception(\"Feature value of zero in output!\")\n",
        "      bundle_string = \",\".join(feat_strings)\n",
        "      if bundle_string in feats2symbol_sr.keys():\n",
        "        seg_string = feats2symbol_sr[bundle_string]\n",
        "      else:\n",
        "        seg_string = \"?\"\n",
        "      if seg_string != \"E\":\n",
        "        word_list.append(seg_string)\n",
        "    word_string = \" \".join(word_list)\n",
        "    if word_string == SRs[i]:\n",
        "      accs_by_word.append(1.0) \n",
        "    else:\n",
        "      accs_by_word.append(0.0) \n",
        "    lang2results[l][URs[i]] = word_string\n",
        "  lang2acc[l] = np.mean(accs_by_word)\n",
        "\n",
        "  #Test data performance\n",
        "  test_predictions = model.predict(test_X)\n",
        "  urs_so_far = []\n",
        "  for i, word in enumerate(test_predictions):\n",
        "    bare_ur = sub(\"_.+\", \"\", URs[i])\n",
        "    if bare_ur in urs_so_far:\n",
        "      continue\n",
        "    urs_so_far.append(bare_ur)  \n",
        "    word_list = []\n",
        "    for seg in word:\n",
        "      feat_strings = []\n",
        "      for feat in seg:\n",
        "        if feat < 0.0:\n",
        "          feat_strings.append(\"-1\")\n",
        "        elif feat > 0.0:\n",
        "          feat_strings.append(\"1\")\n",
        "        else:\n",
        "          raise Exception(\"Feature value of zero in output!\")\n",
        "      bundle_string = \",\".join(feat_strings)\n",
        "      if bundle_string in feats2symbol_sr.keys():\n",
        "        seg_string = feats2symbol_sr[bundle_string]\n",
        "      else:\n",
        "        seg_string = \"?\"\n",
        "      if seg_string != \"E\":\n",
        "        word_list.append(seg_string) \n",
        "    word_string = \" \".join(word_list)\n",
        "    if word_string == SRs[i]:\n",
        "      accs_by_word.append(1.0) \n",
        "    else:\n",
        "      accs_by_word.append(0.0) \n",
        "\n",
        "    lang2testResults[l][bare_ur] = word_string\n",
        "\n",
        "  backend.clear_session()\n",
        "  clear_output()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2d7u75CXqfWO"
      },
      "source": [
        "##Plot and save the results:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "56w3un1_owBJ"
      },
      "source": [
        "l2ur2sr = {l:{} for l in lang_list}\n",
        "for l in lang_list:\n",
        "  SRs = lang2SRs[l]\n",
        "  for raw_ur, sr in zip(URs,SRs):\n",
        "    ur = sub(\"_.+\", \"\", raw_ur)\n",
        "    l2ur2sr[l][ur] = sr\n",
        "\n",
        "for l in lang_list:\n",
        "  type_acc = {}\n",
        "  output_file = open(\"/content/gdrive/My Drive/NN_Stress/Output_Files/\"+str(l)+\"_token_output.csv\", \"w\")\n",
        "  output_file.write(\"UR,Correct_SR,Predicted_SR\\n\")\n",
        "  for ur in lang2results[l].keys():\n",
        "    type_ur = sub(\"_.+\", \"\", ur)\n",
        "    this_sr = l2ur2sr[l][type_ur]\n",
        "    if type_ur in type_acc.keys():\n",
        "      type_acc[type_ur].append(int(lang2results[l][ur]==this_sr))\n",
        "    else:\n",
        "      type_acc[type_ur] = [int(lang2results[l][ur]==this_sr)]\n",
        "    output_file.write(\",\".join([ur, this_sr, lang2results[l][ur]])+\"\\n\")\n",
        "  output_file.close()\n",
        "\n",
        "  type_output_f = open(\"/content/gdrive/My Drive/NN_Stress/Output_Files/\"+str(l)+\"_type_output.csv\", \"w\")\n",
        "  type_output_f.write(\"UR-Type,Accuracy\\n\")\n",
        "  for ur in type_acc.keys():\n",
        "    type_output_f.write(ur+\",\"+str(np.mean(type_acc[ur]))+\"\\n\")\n",
        "  type_output_f.close()\n",
        "\n",
        "  test_output_f = open(\"/content/gdrive/My Drive/NN_Stress/Output_Files/\"+str(l)+\"_test_output.csv\", \"w\")\n",
        "  test_output_f.write(\"Test-UR,Correct-SR,Predicted-SR,Accuracy\\n\")\n",
        "  urs_so_far = []\n",
        "  for ur in lang2testResults[l].keys():\n",
        "    bare_ur = sub(\"_.+\", \"\", ur)\n",
        "    if bare_ur in urs_so_far:\n",
        "      continue\n",
        "    this_sr = l2ur2sr[l][bare_ur]\n",
        "    urs_so_far.append(bare_ur)\n",
        "    test_output_f.write(\",\".join([bare_ur, this_sr, lang2testResults[l][ur], str(int(this_sr==lang2testResults[l][ur]))])+\"\\n\")\n",
        "  test_output_f.close()   \n",
        "\n",
        "success_file = open(\"/content/gdrive/My Drive/NN_Stress/Output_Files/\"+str(lang_list[0])+\"-\"+str(lang_list[-1])+\"_accuracies.csv\", \"w\")\n",
        "success_file.write(\"Language,Accuracy\\n\")\n",
        "for l in lang2acc.keys():\n",
        "  success_file.write(str(l)+\",\"+str(lang2acc[l])+\"\\n\")\n",
        "success_file.close() "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yZ0C8ugw3U7U",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "outputId": "1ea8f826-af6a-43c2-e82e-125383d2d77a"
      },
      "source": [
        "av_curve = np.mean(learning_curves, axis=0)\n",
        "plot(av_curve) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f29fb85a590>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}