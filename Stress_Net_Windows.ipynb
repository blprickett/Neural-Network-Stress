{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Stress-Net-Windows.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zZr8ReFcNBEi"
      },
      "source": [
        "##Hyperparameters for the model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CTlg8GYqplLc",
        "outputId": "257b03aa-2b66-4208-89ec-3fdd0bb5c0ce"
      },
      "source": [
        "max_epochs = 1#000 #Maximum number of epochs\n",
        "learning_rate = 0.005\n",
        "hidden_feat_num = 20 #Was 20, then 40\n",
        "batch_size = 1\n",
        "layers = \"lstm\"\n",
        "\n",
        "lang_list = [1,2]#list(range(1,11)) #This doesn't actually change the language--just tells you how many reps you're doing\n",
        "token_per_type = 4\n",
        "windows = (.75, .25)  #This is a tuple that's N long (where N is the size of the window stress can appear in) \n",
        "                      #with each number representing the proportion of the time that that syllable is stressed.\n",
        "window_side = \"R\" #Will the window be from the right or left side of the word? (\"L\" for left, \"R\" for right)\n",
        "\n",
        "#Dictionary that maps from feature values in the SR to segments\n",
        "feats2symbol_sr = {\n",
        "                      \"1,1\":\"1\",\n",
        "                      \"1,-1\":\"0\",\n",
        "                      \"-1,-1\":\"E\" \n",
        "                  }\n",
        "\n",
        "#Mount the user's google drive:\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4kHGQQcmM9nZ"
      },
      "source": [
        "##Import the necessary packages:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NzGfc-lRpId2",
        "outputId": "aadb9377-d416-4adf-9719-352bc8602882"
      },
      "source": [
        "#CD into our google drive so we can use the custom Seq2Seq package\n",
        "%cd gdrive/My Drive\n",
        "\n",
        "import numpy as np\n",
        "from keras import backend\n",
        "import Seq2Seq\n",
        "\n",
        "from re import sub, search\n",
        "from random import choice, shuffle\n",
        "from itertools import product\n",
        "from matplotlib.pyplot import plot\n",
        "from IPython.display import clear_output\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/My Drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RiyA_Je6NIdU"
      },
      "source": [
        "##Functions for processing the training data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gWYjgS_MqcFX"
      },
      "source": [
        "def get_strings(input_fn, copies=1):\n",
        "  input_file = open(\"/content/gdrive/My Drive/NN_Stress/Input_Files/\"+input_fn)\n",
        "  input_file.readline()\n",
        "  UR_strings = []\n",
        "  SR_strings = []\n",
        "  syll_lengths = []\n",
        "  for line in input_file.readlines():\n",
        "    columns = line.split(\",\")\n",
        "    if len(columns) == 1:\n",
        "      raw_ur = line.rstrip() \n",
        "    if len(columns) == 3:\n",
        "      raw_sr = columns[1]\n",
        "      raw_p = columns[2]\n",
        "      ur, sr, p = sub(\"\\\"|\\[|\\]\", \"\", raw_ur), sub(\"\\[|\\]\", \"\", raw_sr), raw_p.rstrip()\n",
        "      if p == \"1\":\n",
        "        syll_lengths.append(len(ur.split(\" \")))\n",
        "        UR_strings.append(ur)\n",
        "        new_sr = []\n",
        "        for syll in sr.split(\" \"):\n",
        "          new_syll = {\"L1\":\"1\", \"H1\":\"1\", \"L2\":\"2\", \"H2\":\"2\", \"L\":\"0\", \"H\":\"0\"}[syll]\n",
        "          new_sr.append(new_syll)\n",
        "        SR_strings.append(\" \".join(new_sr))\n",
        "\n",
        "  if copies > 1 and len(windows) > 0:\n",
        "    lexical_labels = [c for c in range(1,1+(copies*len(UR_strings)))]\n",
        "    shuffle(lexical_labels)\n",
        "    token_UR_strings = []\n",
        "    token_SR_strings = []\n",
        "    token_syll_lengths = []\n",
        "    for i, ur in enumerate(UR_strings):\n",
        "      old_sr = SR_strings[i]\n",
        "\n",
        "      #Window-based stress:\n",
        "      these_patterns = [] #List of indeces showing where, on each copy the stress happens\n",
        "      for j, p in enumerate(windows):\n",
        "        num_tokens = p*copies\n",
        "        if num_tokens % 1 != 0:\n",
        "          raise Exception(\"Window proportions do not make integers with copy number!\")\n",
        "        for k in range(int(num_tokens)):\n",
        "          these_patterns.append(j)\n",
        "      shuffle(these_patterns)\n",
        "      for copy in range(copies):\n",
        "        this_label = str(lexical_labels.pop())\n",
        "        token_UR_strings.append(ur+\"_\"+this_label)\n",
        "\n",
        "        #Create SR based on window size:\n",
        "        new_sr = []\n",
        "        this_pattern = these_patterns.pop()\n",
        "        if window_side == \"L\":\n",
        "          for j, syll in enumerate(old_sr.split(\" \")):\n",
        "            if j == this_pattern:\n",
        "              new_sr.append(\"1\")\n",
        "            else:\n",
        "              new_sr.append(\"0\")\n",
        "        elif window_side == \"R\":\n",
        "          window_counter = 0\n",
        "          for j, syll in enumerate(old_sr.split(\" \")):\n",
        "            if j >= len(old_sr.split(\" \"))-len(windows) and window_counter == this_pattern:\n",
        "              new_sr.append(\"1\")\n",
        "              window_counter += 1\n",
        "            elif j >= len(old_sr.split(\" \"))-len(windows):\n",
        "              new_sr.append(\"0\")\n",
        "              window_counter += 1\n",
        "            else:\n",
        "              new_sr.append(\"0\")\n",
        "        else:\n",
        "          raise Exception(\"The variable window_side must be 'L' or 'R'!\")\n",
        "\n",
        "        #print(old_sr, \"->\", \" \".join(new_sr), \"...\", this_pattern, \"!!\", ur)\n",
        "        token_SR_strings.append(\" \".join(new_sr))\n",
        "        token_syll_lengths.append(syll_lengths[i])\n",
        "    return token_UR_strings, token_SR_strings, token_syll_lengths\n",
        "\n",
        "  return UR_strings, SR_strings, syll_lengths\n",
        "\n",
        "def get_arrays(UR_strings, SR_strings, syll_lengths):\n",
        "  symbol2feats_ur = {\n",
        "                  #Syll\t#Heavy\n",
        "              \"L\":[\t1.,\t\t-1.],\n",
        "              \"H\":[\t1.,\t\t1.],\n",
        "              \"E\":[\t-1.,\t\t-1.]\n",
        "            }\n",
        "  symbol2feats_sr = {}\n",
        "  for feat_bundle in feats2symbol_sr.keys():\n",
        "    these_feats = [float(f) for f in feat_bundle.split(\",\")]\n",
        "    symbol2feats_sr[feats2symbol_sr[feat_bundle]] = these_feats\n",
        "\n",
        "  if \"_\" in \"\".join(UR_strings):\n",
        "    #If we're using lexical features:\n",
        "    real_strings = []\n",
        "    raw_lex_nums = []\n",
        "    max_lexLen = -1\n",
        "    for ur in UR_strings:\n",
        "      real_string, lex_num = ur.split(\"_\")\n",
        "      real_strings.append(real_string)\n",
        "      bin_lex_num = str(bin(int(lex_num)).replace(\"0b\", \"\"))\n",
        "      raw_lex_nums.append(bin_lex_num)\n",
        "      if len(bin_lex_num) > max_lexLen:\n",
        "        max_lexLen = len(bin_lex_num)\n",
        "    lex_nums = []\n",
        "    for ln in raw_lex_nums:\n",
        "      zeros = \"0\" * (max_lexLen - len(ln))\n",
        "      lex_nums.append([float(digit) for digit in zeros+ln])\n",
        "\n",
        "    max_len = max(syll_lengths)\n",
        "    X_list = []\n",
        "    Y_list = []\n",
        "    for word_index, syll_length in enumerate(syll_lengths):\n",
        "      padding = \" \".join([\"E\"]*(max_len-syll_length))\n",
        "      this_length = len(real_strings[word_index].split(\" \"))\n",
        "      this_ur = real_strings[word_index]+\" \"+padding\n",
        "      this_sr = SR_strings[word_index]+\" \"+padding\n",
        "      X_list.append([symbol2feats_ur[seg]+lex_nums[word_index] for seg in this_ur.split(\" \") if seg != \"\"])\n",
        "      Y_list.append([symbol2feats_sr[seg] for seg in this_sr.split(\" \") if seg != \"\"])\n",
        "  else:\n",
        "    max_len = max(syll_lengths)\n",
        "    X_list = []\n",
        "    Y_list = []\n",
        "    for word_index, syll_length in enumerate(syll_lengths):\n",
        "      padding = \" \".join([\"E\"]*(max_len-syll_length))\n",
        "      this_length = len(UR_strings[word_index].split(\" \"))\n",
        "      this_ur = UR_strings[word_index]+\" \"+padding\n",
        "      this_sr = SR_strings[word_index]+\" \"+padding\n",
        "      X_list.append([symbol2feats_ur[seg] for seg in this_ur.split(\" \") if seg != \"\"])\n",
        "      Y_list.append([symbol2feats_sr[seg] for seg in this_sr.split(\" \") if seg != \"\"])\n",
        "\n",
        "  X = np.array(X_list)\n",
        "  Y = np.array(Y_list)\n",
        "\n",
        "  return X, Y\n",
        "\n",
        "\n",
        "def get_test_arrays (UR_strings, syll_lengths):\n",
        "  symbol2feats_ur = {\n",
        "                #Syll\t#Heavy\n",
        "            \"L\":[\t1.,\t\t-1.],\n",
        "            \"H\":[\t1.,\t\t1.],\n",
        "            \"E\":[\t-1.,\t\t-1.]\n",
        "          }\n",
        "  real_strings = []\n",
        "  raw_lex_nums = []\n",
        "  max_lexLen = -1\n",
        "  for ur in UR_strings:\n",
        "    real_string, lex_num = ur.split(\"_\")\n",
        "    real_strings.append(real_string)\n",
        "    bin_lex_num = str(bin(int(lex_num)).replace(\"0b\", \"\"))\n",
        "    raw_lex_nums.append(bin_lex_num)\n",
        "    if len(bin_lex_num) > max_lexLen:\n",
        "      max_lexLen = len(bin_lex_num)\n",
        "  lex_nums = []\n",
        "  for ln in raw_lex_nums:\n",
        "    zeros = \"0\" * (max_lexLen - len(ln))\n",
        "    lex_nums.append([0.0 for digit in zeros+ln]) #All zeroes for lexical features, b/c test data are nonce words\n",
        "\n",
        "  max_len = max(syll_lengths)\n",
        "  X_list = []\n",
        "  for word_index, syll_length in enumerate(syll_lengths):\n",
        "    padding = \" \".join([\"E\"]*(max_len-syll_length))\n",
        "    this_length = len(real_strings[word_index].split(\" \"))\n",
        "    this_ur = real_strings[word_index]+\" \"+padding\n",
        "    X_list.append([symbol2feats_ur[seg]+lex_nums[word_index] for seg in this_ur.split(\" \") if seg != \"\"])\n",
        "\n",
        "  return np.array(X_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gcw6CW8oNPPv"
      },
      "source": [
        "##Run the simulations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Xez-20vqckP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "504310e7-4ebe-42d3-ec97-d9babe1cd197"
      },
      "source": [
        "learning_curves = []\n",
        "lang2results = {l:{} for l in lang_list}\n",
        "lang2testResults = {l:{} for l in lang_list}\n",
        "lang2acc = {l:0 for l in lang_list}\n",
        "lang2SRs = {}\n",
        "lang2URs = {}\n",
        "lang2epochs = {}\n",
        "failed_reps = []\n",
        "for l in lang_list:\n",
        "  print (\"Rep\", l, \"of\", lang_list[-1], \"... Learning language with\", window_side, \"aligned stress and probabilities of\", windows, \"...\")\n",
        "\n",
        "  #Load the training data:\n",
        "  file_name = \"ts\"+str(l)+\".csv\"\n",
        "  URs, SRs, Ls = get_strings(file_name, token_per_type)\n",
        "  lang2SRs[l] = SRs\n",
        "  lang2URs[l] = URs\n",
        "  X, Y = get_arrays(URs, SRs, Ls)\n",
        "  test_X = get_test_arrays(URs, Ls)\n",
        "  lang2results[l] = {ur:\"\" for ur in URs}\n",
        "  lang2testResults[l] = {sub(\"_.+\", \"\", ur):\"\" for ur in URs}\n",
        "\n",
        "  #Build the model:\n",
        "  model = Seq2Seq.seq2seq(\n",
        "                              input_dim=X.shape[2],\n",
        "                              hidden_dim=hidden_feat_num,\n",
        "                              output_length=Y.shape[1],\n",
        "                              output_dim=Y.shape[2],\n",
        "                              batch_size=batch_size,\n",
        "                              learn_rate=learning_rate,\n",
        "                              layer_type=layers\n",
        "                            )\n",
        "  this_curve = []\n",
        "  for ep in range(max_epochs):\n",
        "    if ep+1 % 10 == 0:\n",
        "      print(\"Epoch\", ep+1)\n",
        "\n",
        "    #Train the model:\n",
        "    hist = model.train(\n",
        "                          X, Y,\n",
        "                          epoch_num=1,\n",
        "                          print_every=10\n",
        "                      )\n",
        "    this_curve += hist[\"Loss\"] \n",
        "\n",
        "    if ep % 100 == 0:\n",
        "      #Check performance on the training data\n",
        "      #to see if we can end the simulation:\n",
        "      Y_hat = model.predict(X)\n",
        "      accs_by_word = []\n",
        "      epoch_num = 0\n",
        "      for i, word in enumerate(Y_hat):\n",
        "        word_list = []\n",
        "        for seg in word:\n",
        "          feat_strings = []\n",
        "          for feat in seg:\n",
        "            if feat < 0.0:\n",
        "              feat_strings.append(\"-1\")\n",
        "            elif feat > 0.0:\n",
        "              feat_strings.append(\"1\")\n",
        "            else:\n",
        "              raise Exception(\"Feature value of zero in output!\")\n",
        "          bundle_string = \",\".join(feat_strings)\n",
        "          if bundle_string in feats2symbol_sr.keys():\n",
        "            seg_string = feats2symbol_sr[bundle_string]\n",
        "          else:\n",
        "            seg_string = \"?\"\n",
        "          if seg_string != \"E\":\n",
        "            word_list.append(seg_string)\n",
        "        word_string = \" \".join(word_list)\n",
        "        if word_string == SRs[i]:\n",
        "          accs_by_word.append(1.0) \n",
        "        else:\n",
        "          accs_by_word.append(0.0) \n",
        "      overall_acc = np.mean(accs_by_word)\n",
        "      \n",
        "      if overall_acc == 1.0:\n",
        "        break\n",
        "      else:\n",
        "        print(\"\\t\"+str(overall_acc*100)+\"% accurate...\")\n",
        "  \n",
        "  if overall_acc != 1.0:\n",
        "    #raise Exception(\"Failed to reach 100% accuracy on training data!\")\n",
        "    #lang_list.append(lang_list[-1]+1)\n",
        "    failed_reps.append(l)\n",
        "    continue\n",
        "  lang2epochs[l] = ep+1\n",
        "      \n",
        "  #Save the useful info and delete everything else:\n",
        "  #Loss in training\n",
        "  learning_curves.append(this_curve)\n",
        "\n",
        "  #Test data performance\n",
        "  test_predictions = model.predict(test_X)\n",
        "  for i, word in enumerate(test_predictions):\n",
        "    word_list = []\n",
        "    for seg in word:\n",
        "      feat_strings = []\n",
        "      for feat in seg:\n",
        "        if feat < 0.0:\n",
        "          feat_strings.append(\"-1\")\n",
        "        elif feat > 0.0:\n",
        "          feat_strings.append(\"1\")\n",
        "        else:\n",
        "          raise Exception(\"Feature value of zero in output!\")\n",
        "      bundle_string = \",\".join(feat_strings)\n",
        "      if bundle_string in feats2symbol_sr.keys():\n",
        "        seg_string = feats2symbol_sr[bundle_string]\n",
        "      else:\n",
        "        seg_string = \"?\"\n",
        "      if seg_string != \"E\":\n",
        "        word_list.append(seg_string)\n",
        "    word_string = \" \".join(word_list)\n",
        "    if word_string == SRs[i]:\n",
        "      accs_by_word.append(1.0) \n",
        "    else:\n",
        "      accs_by_word.append(0.0) \n",
        "    lang2testResults[l][sub(\"_.+\", \"\", URs[i])] = word_string \n",
        "\n",
        "  backend.clear_session() \n",
        "  clear_output()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rep 1 of 2 ... Learning language with R aligned stress and probabilities of (0.75, 0.25) ...\n",
            "\t0.0% accurate...\n",
            "Rep 2 of 2 ... Learning language with R aligned stress and probabilities of (0.75, 0.25) ...\n",
            "\t0.0% accurate...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2d7u75CXqfWO"
      },
      "source": [
        "##Plot and save the results:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "56w3un1_owBJ"
      },
      "source": [
        "for l in lang_list:\n",
        "  if l in failed_reps:\n",
        "    continue\n",
        "  #type_acc = {} \n",
        "  #output_file = open(\"/content/gdrive/My Drive/NN_Stress/Output_Files/\"+window_side+\"(\"+str(windows[0])+\",\"+str(windows[1])+\")\"+\"_token_output_\"+str(l)+\".csv\", \"w\")\n",
        "  #output_file.write(\"UR,Correct_SR,Predicted_SR\\n\")\n",
        "  #for ur, sr in zip(lang2URs[l], lang2SRs[l]):\n",
        "  #  type_ur = sub(\"_.+\", \"\", ur)\n",
        "  #  if type_ur in type_acc.keys():\n",
        "  #    type_acc[type_ur].append(int(lang2results[l][ur]==sr))\n",
        "  #  else:\n",
        "  #    type_acc[type_ur] = [int(lang2results[l][ur]==sr)]\n",
        "  #  output_file.write(\",\".join([ur, sr, lang2results[l][ur]])+\"\\n\")\n",
        "  #output_file.close()\n",
        "\n",
        "  #type_output_f = open(\"/content/gdrive/My Drive/NN_Stress/Output_Files/\"+window_side+\"(\"+str(windows[0])+\",\"+str(windows[1])+\")\"+\"_type_output_\"+str(l)+\".csv\", \"w\")\n",
        "  #type_output_f.write(\"UR-Type,Accuracy\\n\")\n",
        "  #for ur in type_acc.keys():\n",
        "  #  type_output_f.write(ur+\",\"+str(np.mean(type_acc[ur]))+\"\\n\")\n",
        "  #type_output_f.close()\n",
        "\n",
        "  test_output_f = open(\"/content/gdrive/My Drive/NN_Stress/Output_Files/\"+window_side+\"(\"+str(windows[0])+\",\"+str(windows[1])+\")\"+\"_test_output_\"+str(l)+\".csv\", \"w\")\n",
        "  test_output_f.write(\"Test-UR,Predicted-SR,Stressed-Syll\\n\")\n",
        "  urs_so_far = []\n",
        "  for ur in lang2testResults[l].keys():\n",
        "    bare_ur = sub(\"_.+\", \"\", ur)\n",
        "    if bare_ur in urs_so_far:\n",
        "      continue\n",
        "    urs_so_far.append(bare_ur)\n",
        "\n",
        "    #What syllable in the window got stressed?\n",
        "    if search(\"^[0 ]*1[0 ]*$\", lang2testResults[l][ur]):\n",
        "      stressed_syll_i = lang2testResults[l][ur].split(\" \").index(\"1\")\n",
        "      if window_side == \"L\" and stressed_syll_i >= len(windows):\n",
        "        stressed_syll = \"Outside of Window\"\n",
        "      elif window_side == \"R\" and stressed_syll_i < (len(bare_ur.split(\" \")) - len(windows)):\n",
        "        stressed_syll = \"Outside of Window\"\n",
        "      elif window_side == \"L\":\n",
        "        stressed_syll = str(stressed_syll_i)\n",
        "      elif window_side == \"R\":\n",
        "        new_i = stressed_syll_i - (len(bare_ur.split(\" \")) - len(windows)) \n",
        "        stressed_syll = str(new_i)\n",
        "    else:\n",
        "      stressed_syll = \"Wrong number of stressed syllables!\"\n",
        "\n",
        "    test_output_f.write(\",\".join([bare_ur, lang2testResults[l][bare_ur], stressed_syll])+\"\\n\")\n",
        "  test_output_f.close()   \n",
        "\n",
        "#success_file = open(\"/content/gdrive/My Drive/NN_Stress/Output_Files/\"+window_side+\"(\"+str(windows[0])+\",\"+str(windows[1])+\")\"+\"_accuracies.csv\", \"w\")\n",
        "#success_file.write(\"Language,Accuracy\\n\")\n",
        "#for l in lang2acc.keys():\n",
        "#  success_file.write(str(l)+\",\"+str(lang2acc[l])+\"\\n\")\n",
        "#success_file.close()  \n",
        "\n",
        "epoch_file = open(\"/content/gdrive/My Drive/NN_Stress/Output_Files/\"+window_side+\"(\"+str(windows[0])+\",\"+str(windows[1])+\")\"+\"_NumberOfEpochs.csv\", \"w\")\n",
        "epoch_file.write(\"Language,EpochNum\\n\")\n",
        "for l in lang2epochs.keys():\n",
        "  epoch_file.write(str(l)+\",\"+str(lang2epochs[l])+\"\\n\")\n",
        "epoch_file.close()  \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yZ0C8ugw3U7U",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 352
        },
        "outputId": "a6d3280e-bc2d-4b9d-8e0c-4761b33e35bc"
      },
      "source": [
        "max_length = -1\n",
        "for curve in learning_curves:\n",
        "  if len(curve) > max_length:\n",
        "    max_length = len(curve)\n",
        "padded_curves = [c+[c[-1] for pad in range(max_length-len(c))] for c in learning_curves]\n",
        "av_curve = np.mean(padded_curves, axis=0)\n",
        "plot(av_curve) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/numpy/core/fromnumeric.py:3373: RuntimeWarning: Mean of empty slice.\n",
            "  out=out, **kwargs)\n",
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7fa8efe68250>]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}